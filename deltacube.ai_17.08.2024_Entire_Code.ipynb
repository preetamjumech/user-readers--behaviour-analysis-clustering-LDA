{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8df99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = Content_Reading_Behavior()\n",
    "df.head()\n",
    "\n",
    "#------------------- fillna by others in city column\n",
    "df[\"CITY_NAME\"] = df[\"CITY_NAME\"].fillna('others')\n",
    "df[\"COUNTRY_NAME\"] = df[\"COUNTRY_NAME\"].fillna('others')\n",
    "df[\"PAGE_TITLE\"] = df[\"PAGE_TITLE\"].fillna('others')\n",
    "df.isna().sum()\n",
    "subs_na_df = df[df[\"SUBSCRIBER_ACCT\"].isna()]\n",
    "\n",
    "# KDE plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(subs_na_df[\"TOTAL_TIME_ON_PAGE_SECONDS\"], edgecolor='black')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('KDE Plot of Sample Data')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "subs_na_df.describe()\n",
    "subs_na_df[\"SUBSCRIBER_ACCT\"].unique()\n",
    "df[\"SUBSCRIBER_ACCT\"] = df[\"SUBSCRIBER_ACCT\"].fillna('others')\n",
    "df.isna().sum()\n",
    "\n",
    "total_time_on_page_na_df = df[df[\"TOTAL_TIME_ON_PAGE_SECONDS\"].isna()]\n",
    "total_time_on_page_na_df.head()\n",
    "total_time_on_page_na_df[\"SUBSCRIBER_ACCT\"].unique()\n",
    "df[\"TOTAL_TIME_ON_PAGE_SECONDS\"] = df[\"TOTAL_TIME_ON_PAGE_SECONDS\"].fillna(0)\n",
    "df.isna().sum()\n",
    "#---------------------------------creating user type (new,returning, loyal)------------------------\n",
    "\n",
    "# Define a function to assign category based on frequency\n",
    "def assign_category(frequency):\n",
    "  if frequency == 0:\n",
    "    return \"New User\"\n",
    "  elif 1 <= frequency <= 7:\n",
    "    return \"Returning User\"\n",
    "  else:\n",
    "    return \"Loyal User\"\n",
    "\n",
    "# Create a new column named \"category\" and assign values based on the function\n",
    "df['user_category'] = df['FREQUENCY'].apply(assign_category)\n",
    "\n",
    "#-------------------------- subscribe type - -----------------------------\n",
    "def sunscriber_category(subscribe):\n",
    "  if subscribe == 0.0:\n",
    "    return \"Guest User\"\n",
    "  elif subscribe == 1.0:\n",
    "    return \"Registered User\" \n",
    "  else:\n",
    "    return \"Subscribed User\"\n",
    "\n",
    "# Create a new column named \"category\" and assign values based on the function\n",
    "df['subscriber_type'] = df['SUBSCRIBER_ACCT'].apply(sunscriber_category)\n",
    "#----------------- browser ------------------------------\n",
    "browsers = {\n",
    "    \"Chrome\": r\"(?i)Chrome/\",  # Case-insensitive search for \"Chrome/\"\n",
    "    \"Firefox\": r\"(?i)Firefox/\",\n",
    "    \"Safari\": r\"(?i)Safari/\",\n",
    "    \"Edge\": r\"(?i)Edge/\",\n",
    "    \"Opera\": r\"(?i)Opera/\"\n",
    "}\n",
    "import re\n",
    "# Define lambda function with regular expressions\n",
    "def get_browser(user_agent):\n",
    "  for browser, pattern in browsers.items():\n",
    "    if re.search(pattern, user_agent):\n",
    "      return browser\n",
    "  return \"Other\"\n",
    "\n",
    "# Apply lambda function\n",
    "df['browser'] = df['USER_AGENT'].apply(get_browser)\n",
    "\n",
    "#----------------------------- time bin ---------------------------\n",
    "df['date'] = df['LAST_PING_TIMESTAMP'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime(\"%Y-%m-%d\"))\n",
    "df['time'] = df['LAST_PING_TIMESTAMP'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime(\"%H:%M:%S\"))\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['time'] = pd.to_datetime(df['time']).dt.hour\n",
    "\n",
    "# df[\"hour\"] = df[\"time\"].dt.hour\n",
    "# df[\"hour\"]\n",
    "df[\"time\"].tail()\n",
    "bins = [0,1,2,3,4,5,6,7,8,9,10, 11, 12,13, 14, 15, 16, 17, 18, 19, 20, 21, 22,23, 24]\n",
    "# bins = pd.to_timedelta([i * np.timedelta64(1, 'h') for i in range(24)])\n",
    "# bins = [str(t)[7:-3] for t in bins]\n",
    "# # bins = [float(i) for i in bins]\n",
    "# bins\n",
    "# bins = [str(t)[:-3] for t in bins.dt.components]\n",
    "\n",
    "labels = ['00:00-00:59', '01:00-01:59', '02:00-02:59', '03:00-03:59', '04:00-04:59', '05:00-05:59', '06:00-06:59', \n",
    "'07:00-07:59', '08:00-08:59', '09:00-09:59', '10:00-10:59', '11:00-11:59', '12:00-12:59', '13:00-13:59', '14:00-14:59', \n",
    "'15:00-15:59', '16:00-16:59', '17:00-17:59', '18:00-18:59', '19:00-19:59', '20:00-20:59', '21:00-21:59', '22:00-22:59', \n",
    "'23:00-23:59']\n",
    "# # # labels = pd.to_timedelta([pd.to_datetime(t).time() for t in labels])\n",
    "df['time_bin'] = pd.cut(df.time, bins= bins, labels=labels, right=False)\n",
    "\n",
    "# --------------------------------------- topic modeling ---------------------------------\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(headline):\n",
    "    le=WordNetLemmatizer()\n",
    "    word_tokens=nltk.tokenize.word_tokenize(headline)\n",
    "    tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "    cleaned_text=\" \".join(tokens)\n",
    "    return cleaned_text\n",
    "df['cleaned_article']=df['SECTION'].apply(clean_text)\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=list(stop_words),max_features=10000, max_df=0.8, min_df=2)\n",
    "vect_text=vect.fit_transform(df['cleaned_article'])\n",
    "vocab = vect.get_feature_names_out()\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=10,\n",
    "                                    learning_method='online',random_state=42,max_iter=1) \n",
    "lda_top=lda_model.fit_transform(vect_text)\n",
    "\n",
    "dominant_topics = lda_top.argmax(axis=1)\n",
    "df['topic'] = dominant_topics\n",
    "print(df[\"topic\"].nunique())\n",
    "\n",
    "\n",
    "topic_words = {}\n",
    "n_top_words = 5\n",
    "\n",
    "for topic, comp in enumerate(lda_model.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
    "\n",
    "for topic, words in topic_words.items():\n",
    "    print('Topic: %d' % topic)\n",
    "    print('  %s' % ', '.join(words))\n",
    "    \n",
    "df.head()\n",
    "#---------------------------------------- replacing time bin by numbers-------------------\n",
    "df['time_bin'].isna().sum()\n",
    "time_intervals = [\n",
    "    \"00:00-00:59\", \"01:00-01:59\", \"02:00-02:59\", \"03:00-03:59\",\n",
    "    \"04:00-04:59\", \"05:00-05:59\", \"06:00-06:59\", \"07:00-07:59\",\n",
    "    \"08:00-08:59\", \"09:00-09:59\", \"10:00-10:59\", \"11:00-11:59\",\n",
    "    \"12:00-12:59\", \"13:00-13:59\", \"14:00-14:59\", \"15:00-15:59\",\n",
    "    \"16:00-16:59\", \"17:00-17:59\", \"18:00-18:59\", \"19:00-19:59\",\n",
    "    \"20:00-20:59\", \"21:00-21:59\", \"22:00-22:59\", \"23:00-23:59\"\n",
    "]\n",
    "\n",
    "mapping_dict = {interval: str(i) for i, interval in enumerate(time_intervals)}\n",
    "\n",
    "df['time_hour_cat'] = df['time_bin'].map(mapping_dict)\n",
    "\n",
    "df_c = df.copy()\n",
    "del df\n",
    "\n",
    "df_c.drop([\"LAST_PING_TIMESTAMP\",'COOKIE_ID','PAGE_SESSION_ID','NEW_USER','PATH','COUNTRY_NAME',\n",
    "          'UTC_OFFSET_MINUTES','USER_AGENT','RECENCY','FREQUENCY','SECTION',\n",
    "          'PAGE_TITLE','SUBSCRIBER_ACCT',\"date\",\"time\",\"cleaned_article\",\"time_bin\"],axis = 1,inplace = True)\n",
    "print(df_c.shape)\n",
    "categorical_col = ['CITY_NAME','DEVICE','browser','user_category','subscriber_type','topic','time_hour_cat']\n",
    "numerical_col = ['ENGAGED_TIME_ON_PAGE_SECONDS', 'TOTAL_TIME_ON_PAGE_SECONDS', 'PAGE_LOAD_TIME' ]\n",
    "print(df_c.info())\n",
    "\n",
    "#--------------------------- group by -----------------------------------\n",
    "# df_c1 = df_c.iloc[7000:8500, :]\n",
    "gpby = df_c.groupby(categorical_col)[numerical_col].agg(['mean', 'min','max'])\n",
    "gpby.columns = [\"_\".join(col_name).rstrip('_') for col_name in gpby.columns]\n",
    "\n",
    "gpby_ungrouped = gpby.reset_index()\n",
    "# print(gpby_ungrouped.shape)\n",
    "# gpby_ungrouped.isna().sum()\n",
    "gpby_ungrouped.dropna(axis = 0, how = 'any', inplace = True)\n",
    "print(gpby_ungrouped.shape)\n",
    "data = gpby_ungrouped.copy()\n",
    "del gpby_ungrouped \n",
    "data[\"ID\"] = range(len(data))\n",
    "\n",
    "features = data[[\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\",\"ENGAGED_TIME_ON_PAGE_SECONDS_min\",\"ENGAGED_TIME_ON_PAGE_SECONDS_max\",\n",
    "                \"TOTAL_TIME_ON_PAGE_SECONDS_mean\",\"TOTAL_TIME_ON_PAGE_SECONDS_min\",\"TOTAL_TIME_ON_PAGE_SECONDS_max\",\n",
    "                \"PAGE_LOAD_TIME_mean\",\"PAGE_LOAD_TIME_min\",\"PAGE_LOAD_TIME_max\"]]\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()  # Standardize features for better distance calculations\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "clustering = DBSCAN(eps=0.05, min_samples=100)\n",
    "cluster_labels = clustering.fit_predict(scaled_features)\n",
    "data['cluster'] = cluster_labels\n",
    "\n",
    "unique_clusters = np.unique(cluster_labels[cluster_labels != -1])\n",
    "number_of_clusters = len(unique_clusters)\n",
    "\n",
    "data.head()\n",
    "annova_df = data[data[\"cluster\"]!= -1]\n",
    "annova_data = annova_df[[\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\",\"cluster\"]]\n",
    "# annova_data.head()\n",
    "# annova_data[\"cluster\"].unique()\n",
    "# print(annova_data[annova_data[\"cluster\"]==0][\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"].iloc[0])\n",
    "# print(annova_data[annova_data[\"cluster\"]==1][\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"].iloc[0])\n",
    "# print(annova_data[annova_data[\"cluster\"]==2][\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"].iloc[0])\n",
    "# print(annova_data[annova_data[\"cluster\"]==3][\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"].iloc[0])\n",
    "\n",
    "import scipy.stats as stats\n",
    "stats.f_oneway(annova_data['ENGAGED_TIME_ON_PAGE_SECONDS_mean'][annova_data['cluster'] == 0],\n",
    "              annova_data['ENGAGED_TIME_ON_PAGE_SECONDS_mean'][annova_data['cluster'] == 1],\n",
    "              annova_data['ENGAGED_TIME_ON_PAGE_SECONDS_mean'][annova_data['cluster'] == 2],\n",
    "              annova_data['ENGAGED_TIME_ON_PAGE_SECONDS_mean'][annova_data['cluster'] == 3])\n",
    "              \n",
    "annova_df.head()\n",
    "\n",
    "annova_df_0 = annova_df[annova_df[\"cluster\"]==0]\n",
    "c0 = annova_df_0[[\"ID\",\"CITY_NAME\",\"DEVICE\",\"browser\",\"user_category\",\n",
    "                  \"subscriber_type\",\"topic\",\"time_hour_cat\",\n",
    "                  \"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"]].sort_values(by =\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\", \n",
    "                                                                    ascending=False)\n",
    "\n",
    "print(c0.head())\n",
    "\n",
    "annova_df_1 = annova_df[annova_df[\"cluster\"]==1]\n",
    "c1 = annova_df_1[[\"ID\",\"CITY_NAME\",\"DEVICE\",\"browser\",\"user_category\",\n",
    "                  \"subscriber_type\",\"topic\",\"time_hour_cat\",\n",
    "                  \"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"]].sort_values(by =\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\", \n",
    "                                                                    ascending=False)\n",
    "\n",
    "print(c1.head())\n",
    "\n",
    "annova_df_2 = annova_df[annova_df[\"cluster\"]==2]\n",
    "c2 = annova_df_2[[\"ID\",\"CITY_NAME\",\"DEVICE\",\"browser\",\"user_category\",\n",
    "                  \"subscriber_type\",\"topic\",\"time_hour_cat\",\n",
    "                  \"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"]].sort_values(by =\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\", \n",
    "                                                                    ascending=False)\n",
    "\n",
    "print(c2.head())\n",
    "\n",
    "\n",
    "annova_df_3 = annova_df[annova_df[\"cluster\"]==3]\n",
    "c3 = annova_df_3[[\"ID\",\"CITY_NAME\",\"DEVICE\",\"browser\",\"user_category\",\n",
    "                  \"subscriber_type\",\"topic\",\"time_hour_cat\",\n",
    "                  \"ENGAGED_TIME_ON_PAGE_SECONDS_mean\"]].sort_values(by =\"ENGAGED_TIME_ON_PAGE_SECONDS_mean\", \n",
    "                                                                    ascending=False)\n",
    "\n",
    "print(c3.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012c51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
